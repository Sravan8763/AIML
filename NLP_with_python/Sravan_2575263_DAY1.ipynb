{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0da1e1c4",
   "metadata": {},
   "source": [
    "# 1. What is the purpose of text preprocessing in NLP, and why is it essential before analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27da8885",
   "metadata": {},
   "source": [
    "Text preprocessing is an essential step in natural language processing (NLP) that involves cleaning and transforming unstructured text data to prepare it for analysis. It includes tokenization, stemming, lemmatization, stop-word removal, and part-of-speech tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8fff83",
   "metadata": {},
   "source": [
    "# 2. Describe tokenization in NLP and explain its significance in text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a300a8c",
   "metadata": {},
   "source": [
    "Tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning. The first step of the NLP process is gathering the data (a sentence) and breaking it into understandable parts (words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e8f2db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " Tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning. The first step of the NLP process is gathering the data (a sentence) and breaking it into understandable parts (words).\n",
      "\n",
      "Aftr sentence Tokanization:\n",
      " ['Tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning.', 'The first step of the NLP process is gathering the data (a sentence) and breaking it into understandable parts (words).']\n",
      "\n",
      "no of sentnces:\n",
      " 2\n",
      "======================================================================\n",
      "\n",
      "Original text:\n",
      " Tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning. The first step of the NLP process is gathering the data (a sentence) and breaking it into understandable parts (words).\n",
      "\n",
      "Aftr word Tokanization:\n",
      " ['Tokenization', 'is', 'used', 'in', 'natural', 'language', 'processing', 'to', 'split', 'paragraphs', 'and', 'sentences', 'into', 'smaller', 'units', 'that', 'can', 'be', 'more', 'easily', 'assigned', 'meaning', '.', 'The', 'first', 'step', 'of', 'the', 'NLP', 'process', 'is', 'gathering', 'the', 'data', '(', 'a', 'sentence', ')', 'and', 'breaking', 'it', 'into', 'understandable', 'parts', '(', 'words', ')', '.']\n",
      "\n",
      "no of words:\n",
      " 48\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text=\"\"\"Tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning. The first step of the NLP process is gathering the data (a sentence) and breaking it into understandable parts (words).\"\"\"\n",
    "print('Original text:\\n',text)\n",
    "print()\n",
    "tokenised_sent=sent_tokenize(text)\n",
    "print('Aftr sentence Tokanization:\\n',tokenised_sent)\n",
    "print()\n",
    "print('no of sentnces:\\n',len(tokenised_sent))\n",
    "print('='*70)\n",
    "print()\n",
    "\n",
    "print('Original text:\\n',text)\n",
    "print()\n",
    "tokenised_word=word_tokenize(text)\n",
    "print('Aftr word Tokanization:\\n',tokenised_word)\n",
    "print()\n",
    "print('no of words:\\n',len(tokenised_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22dad9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sravan\n",
      "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f22f8f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sravan\n",
      "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edec7742",
   "metadata": {},
   "source": [
    "# What are the differences between stemming and lemmatization in NLP? When would you choose one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc7be02",
   "metadata": {},
   "source": [
    "Stemming is a process that stems or removes last few characters from a word, often leading to incorrect meanings and spelling. Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma. For instance, stemming the word 'Caring' would return 'Car'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "180a5809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Tokenized words - without stemming:\n",
      "\n",
      "\t ['Tokenization', 'is', 'used', 'in', 'natural', 'language', 'processing', 'to', 'split', 'paragraphs', 'and', 'sentences', 'into', 'smaller', 'units', 'that', 'can', 'be', 'more', 'easily', 'assigned', 'meaning', '.', 'The', 'first', 'step', 'of', 'the', 'NLP', 'process', 'is', 'gathering', 'the', 'data', '(', 'a', 'sentence', ')', 'and', 'breaking', 'it', 'into', 'understandable', 'parts', '(', 'words', ')', '.']\n",
      "======================================================================\n",
      "\n",
      "Tokenized words - afer stemming are:\n",
      "\t ['token', 'is', 'use', 'in', 'natur', 'languag', 'process', 'to', 'split', 'paragraph', 'and', 'sentenc', 'into', 'smaller', 'unit', 'that', 'can', 'be', 'more', 'easili', 'assign', 'mean', '.', 'the', 'first', 'step', 'of', 'the', 'nlp', 'process', 'is', 'gather', 'the', 'data', '(', 'a', 'sentenc', ')', 'and', 'break', 'it', 'into', 'understand', 'part', '(', 'word', ')', '.']\n",
      "======================================================================\n",
      "lemmarized words:\n",
      " ['Tokenization', 'be', 'use', 'in', 'natural', 'language', 'process', 'to', 'split', 'paragraph', 'and', 'sentence', 'into', 'smaller', 'units', 'that', 'can', 'be', 'more', 'easily', 'assign', 'mean', '.', 'The', 'first', 'step', 'of', 'the', 'NLP', 'process', 'be', 'gather', 'the', 'data', '(', 'a', 'sentence', ')', 'and', 'break', 'it', 'into', 'understandable', 'part', '(', 'word', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps=PorterStemmer()\n",
    "\n",
    "stemmed_words=[]\n",
    "\n",
    "for w in tokenised_word:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "    \n",
    "\n",
    "print('='*70)\n",
    "print('Tokenized words - without stemming:\\n\\n\\t',tokenised_word)\n",
    "print('='*70)\n",
    "print('\\nTokenized words - afer stemming are:\\n\\t',stemmed_words)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma=WordNetLemmatizer()\n",
    "\n",
    "lemma_words=[lemma.lemmatize(word,pos='v') for word in tokenised_word ]\n",
    "\n",
    "print('='*70)\n",
    "print('lemmarized words:\\n',lemma_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17562c2",
   "metadata": {},
   "source": [
    "# 4. Explain the concept of stop words and their role in text preprocessing. How do they impact NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686e72b7",
   "metadata": {},
   "source": [
    "Stop words are a set of commonly used words in any language. For example, in English, “the”, “is” and “and”, would easily qualify as stop words. In NLP and text mining applications, stop words are used to eliminate unimportant words, allowing applications to focus on the important words instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9f4c600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sravan\n",
      "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cbee1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of words:\t 48\n",
      "======================================================================\n",
      "Tokenized words - with stop words:\n",
      "\n",
      "\t ['Tokenization', 'is', 'used', 'in', 'natural', 'language', 'processing', 'to', 'split', 'paragraphs', 'and', 'sentences', 'into', 'smaller', 'units', 'that', 'can', 'be', 'more', 'easily', 'assigned', 'meaning', '.', 'The', 'first', 'step', 'of', 'the', 'NLP', 'process', 'is', 'gathering', 'the', 'data', '(', 'a', 'sentence', ')', 'and', 'breaking', 'it', 'into', 'understandable', 'parts', '(', 'words', ')', '.']\n",
      "======================================================================\n",
      "Length after the remoal of stopwords:\t 31\n",
      "======================================================================\n",
      "\n",
      "Tokenized words - afer removing the stopwords are:\n",
      "\t ['Tokenization', 'used', 'natural', 'language', 'processing', 'split', 'paragraphs', 'sentences', 'smaller', 'units', 'easily', 'assigned', 'meaning', '.', 'The', 'first', 'step', 'NLP', 'process', 'gathering', 'data', '(', 'sentence', ')', 'breaking', 'understandable', 'parts', '(', 'words', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words('english'))\n",
    "\n",
    "filtered_tokens=[]\n",
    "for w in tokenised_word:\n",
    "    if w not in stop_words:\n",
    "        filtered_tokens.append(w)\n",
    "print('Length of words:\\t',len(tokenised_word))\n",
    "print('='*70)\n",
    "print('Tokenized words - with stop words:\\n\\n\\t',tokenised_word)\n",
    "print('='*70)\n",
    "print('Length after the remoal of stopwords:\\t',len(filtered_tokens))\n",
    "print('='*70)\n",
    "print('\\nTokenized words - afer removing the stopwords are:\\n\\t',filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c672766",
   "metadata": {},
   "source": [
    "# 5. How does the process of removing punctuation contribute to text preprocessing in NLP? What are its benefits?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c744a9b2",
   "metadata": {},
   "source": [
    "The second most common text processing technique is removing punctuations from the textual data. The punctuation removal process will help to treat each text equally. For example, the word data and data! are treated equally after the process of removal of punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fadab380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " Tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning. The first step of the NLP process is gathering the data (a sentence) and breaking it into understandable parts (words).\n",
      "======================================================================\n",
      "after removing puntuations:\n",
      "\n",
      "['Tokenization', 'is', 'used', 'in', 'natural', 'language', 'processing', 'to', 'split', 'paragraphs', 'and', 'sentences', 'into', 'smaller', 'units', 'that', 'can', 'be', 'more', 'easily', 'assigned', 'meaning', 'The', 'first', 'step', 'of', 'the', 'NLP', 'process', 'is', 'gathering', 'the', 'data', 'a', 'sentence', 'and', 'breaking', 'it', 'into', 'understandable', 'parts', 'words']\n"
     ]
    }
   ],
   "source": [
    "words=[word for word in tokenised_word if word.isalpha()]\n",
    "\n",
    "print('Original text:\\n',text)\n",
    "print('='*70)\n",
    "\n",
    "print('after removing puntuations:\\n')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e3bc26",
   "metadata": {},
   "source": [
    "# 6. Discuss the importance of lowercase conversion in text preprocessing. Why is it a common step in NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3edac0d",
   "metadata": {},
   "source": [
    "Lower casing: Converting a word to lower case (NLP -> nlp). Words like Book and book mean the same but when not converted to the lower case those two are represented as two different words in the vector space model (resulting in more dimensions).\n",
    "    \n",
    "The common approach is to reduce everything to lower case for simplicity. Lowercasing is applicable to most text mining and NLP tasks and significantly helps with consistency of the output. However, it is important to remember that some words, like “US” and “us”, can change meanings when reduced to the lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "706de08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " Tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning. The first step of the NLP process is gathering the data (a sentence) and breaking it into understandable parts (words).\n",
      "======================================================================\n",
      "after lowering the case:\n",
      "\n",
      "['tokenization', 'is', 'used', 'in', 'natural', 'language', 'processing', 'to', 'split', 'paragraphs', 'and', 'sentences', 'into', 'smaller', 'units', 'that', 'can', 'be', 'more', 'easily', 'assigned', 'meaning', '.', 'the', 'first', 'step', 'of', 'the', 'nlp', 'process', 'is', 'gathering', 'the', 'data', '(', 'a', 'sentence', ')', 'and', 'breaking', 'it', 'into', 'understandable', 'parts', '(', 'words', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "lower_words=[word.lower() for word in tokenised_word]\n",
    "print('Original text:\\n',text)\n",
    "print('='*70)\n",
    "\n",
    "print('after lowering the case:\\n')\n",
    "print(lower_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea97a505",
   "metadata": {},
   "source": [
    "# 7. Explain the term \"vectorization\" concerning text data. How does techniques like CountVectorizer contribute to text preprocessing in NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3b5e09",
   "metadata": {},
   "source": [
    "In the context of natural language processing (NLP), vectorization refers to the process of converting text into numerical representations, typically vectors of real numbers. This transformation allows computers to understand and process text data, which is crucial for various NLP tasks such as machine translation, sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "790ad83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:\n",
      " ['and' 'assigned' 'be' 'breaking' 'can' 'data' 'easily' 'first'\n",
      " 'gathering' 'in' 'into' 'is' 'it' 'language' 'meaning' 'more' 'natural'\n",
      " 'nlp' 'of' 'paragraphs' 'parts' 'process' 'processing' 'sentence'\n",
      " 'sentences' 'smaller' 'split' 'step' 'that' 'the' 'to' 'tokenization'\n",
      " 'understandable' 'units' 'used' 'words']\n",
      "**********************************************************************\n",
      "Token counts matrix:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vector=CountVectorizer()\n",
    "\n",
    "x=vector.fit_transform(tokenised_word)\n",
    "\n",
    "feature_names=vector.get_feature_names_out()\n",
    "\n",
    "print('Feature names:\\n',feature_names)\n",
    "print('*'*70)\n",
    "print('Token counts matrix:')\n",
    "print(x.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e7200d",
   "metadata": {},
   "source": [
    "# 8. Describe the concept of normalization in NLP. Provide examples of normalization techniques used in text preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe3190f",
   "metadata": {},
   "source": [
    "Normalization in natural language processing (NLP) refers to the process of transforming text data into a consistent and standardized format. This is done to reduce the variability and redundancy in text, making it easier for NLP algorithms to process and analyze. Normalization techniques are essential for text preprocessing, as they prepare the data for downstream tasks such as machine translation, sentiment analysis, and text classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
